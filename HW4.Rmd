---
title: "HW4_Statistical Machine Learning"
author: "Moran Guo"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1 (8.4-10)

### (a)

```{r import data and log-transform}
# Load required packages
library(ISLR)

data <- Hitters
data <- data[!is.na(data$Salary), ]
data$Salary <- log(data$Salary)

head(data)
```

### (b)

```{r train test split}
set.seed(644)

training_set <- data[1:200, ]
test_set <- data[201:nrow(data), ]

dim(training_set)
dim(test_set)
```

### (c)

```{r boosting}
library(gbm)
shrinkage_val <- c(0.0001, 0.001, 0.01, 0.02, 0.03,  0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 1)

training_mse <- numeric(length(shrinkage_val))

for (i in seq_along(shrinkage_val)) {
  boost_model <- gbm(
    formula = Salary ~ ., 
    data = training_set, 
    distribution = "gaussian", 
    n.trees = 1000, 
    shrinkage = shrinkage_val[i]
  )
  train_pred <- predict(boost_model, training_set, n.trees = 1000)
  training_mse[i] <- mean((train_pred - training_set$Salary)^2)
}

plot(shrinkage_val, training_mse, type = 'b', pch = 19, col = "lightblue",
     xlab = "Shrinkage Parameter (位)", ylab = "Training Set MSE",
     main = "Training Set MSE vs. Shrinkage Parameter (位)")
```

### (d)

```{r test set boosting}

test_mse <- numeric(length(shrinkage_val))

for (i in seq_along(shrinkage_val)) {

  boost_model <- gbm(
    formula = Salary ~ ., 
    data = training_set, 
    distribution = "gaussian", 
    n.trees = 1000, 
    shrinkage = shrinkage_val[i]
  )

  test_pred <- predict(boost_model, test_set, n.trees = 1000)
  test_mse[i] <- mean((test_pred - test_set$Salary)^2)
}

results <- data.frame(Shrinkage = shrinkage_val, Test_MSE = test_mse)

plot(shrinkage_val, test_mse, type = "b", pch = 19, col = "lightblue",
     xlab = "Shrinkage Parameter (位)", ylab = "Test Set MSE",
     main = "Test Set MSE vs. Shrinkage Parameter (位)")

results
```

### (e)

I will fit a **simple linear regression model** and a **Ridge regression model**, and then calculate & compare test MSE.

```{r compare MSE}
# Load required packagge
library(glmnet)

# Simple linear regression model
linear_model <- lm(Salary ~ ., data = training_set)

linear_pred <- predict(linear_model, newdata = test_set)

linear_test_mse <- mean((linear_pred - test_set$Salary)^2)

print(paste("Linear Regression Test MSE:", linear_test_mse))

# Ridge regression model

x_train <- model.matrix(Salary ~ ., data = training_set)[, -1]
y_train <- training_set$Salary
x_test <- model.matrix(Salary ~ ., data = test_set)[, -1]

ridge_model <- cv.glmnet(x_train, y_train, alpha = 0, standardize = TRUE)

ridge_pred <- predict(ridge_model, s = ridge_model$lambda.min, newx = x_test)

ridge_test_mse <- mean((ridge_pred - test_set$Salary)^2)
print(paste("Ridge Regression Test MSE:", ridge_test_mse))
```

Based on the results of test MSE, we can observe that for the optimal value of $\lambda \ (0.01)$, the boosting method on the test set performed better than both simple linear regression & Ridge regression model.

### (f)

```{r variable importance}
importance <- summary(boost_model)
print(importance)

summary(boost_model, cBars = 6, las = 1, main = "Variable Importance")
```

From both the importance table & the figure, we can conclude that variables `CRBI`, `Assists`, `PutOuts` are the three most important ones.

### (g)

```{r bagging}
library(randomForest)

set.seed(999)

bagging_model <- randomForest(Salary ~ ., data = training_set, mtry = ncol(training_set) - 1, ntree = 1000, importance = TRUE)

bagging_pred <- predict(bagging_model, newdata = test_set)
bagging_test_mse <- mean((bagging_pred - test_set$Salary)^2)
print(paste("Bagging Test MSE:", bagging_test_mse))
```

So the MSE of Bagging is approximately $0.2316$.

## Question 2 (9.7-7)

### (a)

```{r import data}
# Load required packages
library(ISLR)

data_auto <- Auto
mpg_median <- median(Auto$mpg)
data_auto$mpg_high <- ifelse(Auto$mpg > mpg_median, 1, 0)
head(data_auto)
```

### (b)

```{r svm}
# Load required packages
library(e1071)

set.seed(1234)

data_auto$mpg_high <- as.factor(data_auto$mpg_high)


svm_tune <- tune(svm, mpg_high ~ ., data = data_auto, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 10, 100)))

summary(svm_tune)

best <- svm_tune$best.model
summary(best)
```

Based on the error of the cross-validation (and the summaries), we conclude that `cost = 1` leads to the lowest error rate and smallest dispersion.

### (c)

```{r radial kernel}
# Load required packages
library(e1071)

set.seed(1001)

cost <- c(0.001, 0.01, 0.1, 1, 10, 100)
gamma <- c(0.0001, 0.001, 0.1, 0.5, 1, 2, 3, 4)

svm_tune_rad <- tune(svm, mpg_high ~ ., data = data_auto, kernel = "radial", ranges = list(cost = cost, gamma = gamma))

summary(svm_tune_rad)

best <- svm_tune_rad$best.model
summary(best)
```

```{r degree kernel}
# Load required packages
library(e1071)

degrees <- c(2, 3, 4)

svm_tune_poly <- tune(svm, mpg_high ~ ., data = data_auto, kernel = "polynomial", ranges = list(cost = cost, degree = degrees))

summary(svm_tune_poly)
```

From the results, we can see that for the SVM radial kernel fit, the optimal parameters are `cost = 10` and $\gamma = 0.1$; for the SVM polynomial kernel fit, the optimal parameters are `cost = 100` and `degree = 2`.


### (d)

``` {r svm plots}
library(e1071)

best_radial_svm <- svm_tune_rad$best.model
best_poly_svm <- svm_tune_poly$best.model

#par(mar = c(2, 2, 2, 1), bg = "white") 

custom_palette <- colorRampPalette(c("white", "pink", "lightblue"))

plot(best_radial_svm, data_auto, mpg ~ horsepower,
     main = "Radial Kernel: mpg vs. horsepower",
     symbolPalette = c("red", "blue"),
     color.palette = custom_palette,
     svSymbol = 16, dataSymbol = 1, grid = 100)

plot(best_radial_svm, data_auto, mpg ~ weight,
     main = "Radial Kernel: mpg vs. weight",
     symbolPalette = c("red", "blue"),
     color.palette = custom_palette,
     svSymbol = 16, dataSymbol = 1, grid = 100)

plot(best_radial_svm, data_auto, mpg ~ acceleration,
     main = "Radial Kernel: mpg vs. acceleration",
     symbolPalette = c("red", "blue"),
     color.palette = custom_palette,
     svSymbol = 16, dataSymbol = 1, grid = 100)

plot(best_poly_svm, data_auto, mpg ~ horsepower,
     main = "Polynomial Kernel: mpg vs. horsepower",
     symbolPalette = c("green", "purple"),
     color.palette = custom_palette,
     svSymbol = 16, dataSymbol = 1, grid = 100)

plot(best_poly_svm, data_auto, mpg ~ weight,
     main = "Polynomial Kernel: mpg vs. weight",
     symbolPalette = c("green", "purple"),
     color.palette = custom_palette,
     svSymbol = 16, dataSymbol = 1, grid = 100)

plot(best_poly_svm, data_auto, mpg ~ acceleration,
     main = "Polynomial Kernel: mpg vs. acceleration",
     symbolPalette = c("green", "purple"),
     color.palette = custom_palette,
     svSymbol = 16, dataSymbol = 1, grid = 100)

```


