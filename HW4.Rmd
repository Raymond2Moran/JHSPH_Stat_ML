---
title: "HW4_Statistical Machine Learning"
author: "Moran Guo"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1 (8.4-10)

### (a)

```{r import data and log-transform}
# Load required packages
library(ISLR)

data <- Hitters
data <- data[!is.na(data$Salary), ]
data$Salary <- log(data$Salary)

head(data)
```

### (b)

```{r train test split}
set.seed(644)

training_set <- data[1:200, ]
test_set <- data[201:nrow(data), ]

dim(training_set)
dim(test_set)
```

### (c)

```{r boosting}
library(gbm)
shrinkage_val <- c(0.0001, 0.001, 0.01, 0.02, 0.03, 0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 1)

training_mse <- numeric(length(shrinkage_val))

for (i in seq_along(shrinkage_val)) {
  boost_model <- gbm(
    formula = Salary ~ ., 
    data = training_set, 
    distribution = "gaussian", 
    n.trees = 1000, 
    shrinkage = shrinkage_val[i]
  )
  train_pred <- predict(boost_model, training_set, n.trees = 1000)
  training_mse[i] <- mean((train_pred - training_set$Salary)^2)
}

plot(shrinkage_val, training_mse, type = 'b', pch = 19, col = "lightblue",
     xlab = "Shrinkage Parameter (位)", ylab = "Training Set MSE",
     main = "Training Set MSE vs. Shrinkage Parameter 位")
```

### (d)

```{r test set boosting}

test_mse <- numeric(length(shrinkage_val))

for (i in seq_along(shrinkage_val)) {

  boost_model <- gbm(
    formula = Salary ~ ., 
    data = training_set, 
    distribution = "gaussian", 
    n.trees = 1000, 
    shrinkage = shrinkage_val[i]
  )

  test_pred <- predict(boost_model, test_set, n.trees = 1000)
  test_mse[i] <- mean((test_pred - test_set$Salary)^2)
}

results <- data.frame(Shrinkage = shrinkage_val, Test_MSE = test_mse)

plot(shrinkage_val, test_mse, type = "b", pch = 19, col = "lightblue",
     xlab = "Shrinkage Parameter (位)", ylab = "Test Set MSE",
     main = "Test Set MSE vs. Shrinkage Parameter 位")

results
```

### (e)

I will fit a **simple linear regression model** and a **Ridge regression model**, and then calculate & compare test MSE.

```{r compare MSE}
# Load required packagge
library(glmnet)

# Simple linear regression model
linear_model <- lm(Salary ~ ., data = training_set)

linear_pred <- predict(linear_model, newdata = test_set)

linear_test_mse <- mean((linear_pred - test_set$Salary)^2)

print(paste("Linear Regression Test MSE:", linear_test_mse))

# Ridge regression model

x_train <- model.matrix(Salary ~ ., data = training_set)[, -1]
y_train <- training_set$Salary
x_test <- model.matrix(Salary ~ ., data = test_set)[, -1]

ridge_model <- cv.glmnet(x_train, y_train, alpha = 0, standardize = TRUE)

ridge_pred <- predict(ridge_model, s = ridge_model$lambda.min, newx = x_test)

ridge_test_mse <- mean((ridge_pred - test_set$Salary)^2)
print(paste("Ridge Regression Test MSE:", ridge_test_mse))
```

Based on the results of test MSE, we can observe that for the optimal value of $\lambda \ (0.01)$, the boosting method on the test set performed better than both simple linear regression & Ridge regression model.

### (f)

```{r variable importance}
importance <- summary(boost_model)
print(importance)

#summary(boost_model, cBars = 6, las = 1, main = "Variable Importance")
```

From both the importance table & the figure, we can conclude that variables `CAtBat`, `PutOuts`, `Walks` are the three most important ones.

### (g)

```{r bagging}
library(randomForest)

set.seed(999)

bagging_model <- randomForest(Salary ~ ., data = training_set, mtry = ncol(training_set) - 1, ntree = 1000, importance = TRUE)

bagging_pred <- predict(bagging_model, newdata = test_set)
bagging_test_mse <- mean((bagging_pred - test_set$Salary)^2)
print(paste("Bagging Test MSE:", bagging_test_mse))
```

So the MSE of Bagging is approximately $0.2316$, this is lower than the value of Boosting, Linear & Ridge Regression.

## Question 2 (9.7-7)

### (a)

```{r import data}
# Load required packages
library(ISLR)

data_auto <- Auto
mpg_median <- median(Auto$mpg)
data_auto$mpg_high <- ifelse(Auto$mpg > mpg_median, 1, 0)
head(data_auto)
```

### (b)

Based on previous result of the `Auto` dataset, I intuitively choose variables `displacement` and `weight` to train and build an SVM classifier to separate high and low mpg cars.

```{r svm}
# Load required packages
library(e1071)

set.seed(1234)

data_auto$mpg_high <- as.factor(data_auto$mpg_high)


svm_linear_tune <- tune(svm, mpg_high ~ displacement + weight, data = data_auto, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 10, 100)))

summary(svm_linear_tune)
```

Based on the error of the cross-validation (and the summaries), we conclude that `cost = 0.1` leads to the lowest error rate. Cost value lower than $0.1$ leads to higher error rates, suggesting evidence of underfitting; while higher cost values indicating overfitting since the model is too complex to account for the testing data.

### (c)

```{r radial kernel}
# Load required packages
library(e1071)

set.seed(1001)

cost <- c(0.001, 0.01, 0.1, 1, 10, 100)
gamma <- c(0.0001, 0.001, 0.1, 0.5, 1, 2, 3, 4)

svm_tune_rad <- tune(svm, mpg_high ~ displacement + weight, data = data_auto, kernel = "radial", ranges = list(cost = cost, gamma = gamma))

summary(svm_tune_rad)

best <- svm_tune_rad$best.model
summary(best)
```

```{r degree kernel}
# Load required packages
library(e1071)

degrees <- c(2, 3, 4)

svm_tune_poly <- tune(svm, mpg_high ~ displacement + weight, data = data_auto, kernel = "polynomial", ranges = list(cost = cost, degree = degrees))

summary(svm_tune_poly)
```

From the results, we can see that for the SVM radial kernel fit, the optimal parameters are `cost = 100` and $\gamma = 0.1$; for the SVM polynomial kernel fit, the optimal parameters are `cost = 0.1` and `degree = 3`. Moreover, we could see that the optimal error rates for radial SVM ($0.0945$) is the smallest (thus the best) compared to linear ($0.0969$) and polynomial SVM ($0.1581$), although the error rate difference between the radial and linear SVM could be negligible. I thus conclude that radial SVM models the relationship between `mpg_high` and the rest variables better. This is likely to happen if the relationship of car's mpg and its displacement & weight is not so much linear; however, we need to keep in mind that the linear SVM performed almost equally well compared to the radial kernel. The poor performance of polynomial kernel, on the other hand, suggests that there might not exist a high-order relationship between car's mpg and its displacement & weight.

### (d)

Based on previous output:

The best model for **linear kernel** is: cost = $0.1$;

The best model for **radial kernel** is: cost = $100$, $\gamma = 0.1$;

The best model for **polynomial kernel** is: cost = $0.1$, degree = $3$.


``` {r svm plots}
library(e1071)

# Best linear model
best_linear_svm <- svm(mpg_high ~ displacement + weight, data = data_auto, kernel = "linear", cost = 0.1)

plot(best_linear_svm, data_auto, displacement ~ weight)

# Best radial model
best_radial_svm <- svm(mpg_high ~ displacement + weight, data = data_auto, kernel = "radial", cost = 100, gamma = 0.1)
                   
plot(best_radial_svm, data_auto, displacement ~ weight)

# Best polynomial model
best_polynomial_svm <- svm(mpg_high ~ displacement + weight, data = data_auto, kernel = "polynomial", cost = 0.1, degree = 3)
                       
plot(best_polynomial_svm, data_auto, displacement ~ weight)
```

**Discussion**:

All three plot suggest a negative relationship between car's displacement & mpg and also car's weight & mpg. This aligns with our previous discoveries of the dataset using correlation matrix. Looking at the decision boundaries, it seems that the radial SVM is the most accurate one, since it has a relatively convergent & accurate boundary compared to the rest two. Polynomial kernel drawn the boundary too conservative, so that it mis-classified a lot of high-mpg data point into the lower half. Generally, the SVM classification plots using the best parameters support our previous conclusion: radial kernel performs the best, but linear kernel is also good in separating the two categories.



## Question 3 (9.7-8)

### (a)

```{r manipulate data}
# Load required packages
library(ISLR)

library(dplyr)

data("OJ")

set.seed(233)

train_index <- sample(seq_len(nrow(OJ)), size = 800)

train_set <- OJ[train_index, ]  
test_set <- OJ[-train_index, ]  

print(nrow(train_set))
print(nrow(test_set))
```

### (b)

```{r svm fit oj}
library(e1071)

svm_fit <- svm(Purchase ~ ., data = train_set, kernel = "linear", cost = 0.01, scale = TRUE)

summary(svm_fit)
```

### (c)

```{r svm train test error}
train_preds <- predict(svm_fit, train_set)
train_error_rate <- mean(train_preds != train_set$Purchase)
cat("Training error rate:", train_error_rate, "\n")

test_preds <- predict(svm_fit, test_set)
test_error_rate <- mean(test_preds != test_set$Purchase)
cat("Test error rate:", test_error_rate, "\n")

```

### (d)

```{r fine tune cost}
set.seed(1342)
tune_svm <- tune(
  svm, 
  Purchase ~ ., 
  data = train_set,
  kernel = "linear",
  ranges = list(cost = c(0.001, 0.01, 0.05, 0.1, 1, 2, 5, 10))
)

summary(tune_svm)
```

Based on the tune results, we can see that `cost` equals to $10$ achieved the smallest error rates.

### (e)

We use `cost = 10` to compute the training and test error rates.

```{r svm adjusted cost}

svm_best <- tune_svm$best.model

train_pred_best <- predict(svm_best, train_set)
train_error_best <- mean(train_pred_best != train_set$Purchase)
cat("Training error rate for the best linear SVM model:", train_error_rate, "\n")

test_pred_best <- predict(svm_best, test_set)
test_error_best <- mean(test_pred_best != test_set$Purchase)
cat("Testing set error rate for the best linear SVM model:", test_error_best, "\n")
```

We can see that after applying the optimal cost $10$, training error rate remained the same, while testing error slightly increased from $0.1222222$ to $0.1259$. This might suggest `cost = 10` causes the model to slightly overfit on the training data.

