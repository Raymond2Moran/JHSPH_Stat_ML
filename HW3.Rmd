---
title: "HW3_Statistical Machine Learning"
author: "Moran Guo"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1 (10.7-8)

### (a)

```{r pve output}
# Load required packages
library(ISLR)

pr_out <- prcomp(USArrests, center = TRUE, scale = TRUE)
pve <- pr_out$sdev^2 / sum(pr_out$sdev^2) # Eq. (10.8)
pve
```

### (b)

<mark>To obtain the PVE directly, we consider the equation $(10.8)$. The numerator indicates that we need to multiply the loading vector matrix with the scaled/non-scaled data matrix, take the square of the product, and finally obtain the column summation. The denominator is just the summation of data matrix squared.

```{r pve output alternative}
loading <- pr_out$rotation # Obtain the loading vector
data_m <- scale(USArrests)
pve_alt <- colSums((as.matrix(data_m) %*% loading)^2)/sum(data_m^2)
pve_alt
```

<mark>We can see that the two options for calculating PVE yield the same results.

## Question 2 (10.7-9)

### (a)

```{r complete hierachical clustering}
# Load required packages 
library(ISLR)

set.seed(644)

hc_complete <- hclust(dist(USArrests), method = "complete")

plot(hc_complete)
```

### (b)

```{r cut dendrogram into three clusters}
hc_cut_3 <- cutree(hc_complete, 3)
hc_cut_3
```

### (c)

```{r hierachical with scaling}

data_scaled <- scale(USArrests, center = FALSE)

hc_scaled <- hclust(dist(data_scaled), method = "complete")

plot(hc_scaled)
```

### (d)

<mark>We check the labeled clusters for each state and their agreement:

```{r check labeled clusters}
hc_scaled_cut_3 <- cutree(hc_scaled, 3)

hc_scaled_cut_3

table(hc_scaled_cut_3, hc_cut_3)

USArrests
```

<mark>Observing the `USArrests` dataset, we found that the `Assault` column representing assault rates inherently has larger variance than the other two features, thus the pairwise Euclidean distance is dominated by `Assault` variable. Therefore, scaling is necessary. Scaling ensures that all four variables have similar contributions to the Euclidean distances, and that's the reason why we are seeing discrepancies between the two hierarchical clustering results. 

<mark>So I conclude that the variable should be scaled based on the above reasons.

## Question 3 (10.7-11)

### (a)

```{r import csv}
setwd("~/JHSPH_Stat_ML/")
data <- read.csv("./Ch10Ex11.csv", header = FALSE)
#data
```

### (b)

```{r hierachical clustering complete}
correlation_d <- as.dist(1 - cor(data))

sample_status <- gl(2, 20) # label vectors

hc_complete <- hclust(correlation_d, method = "complete")
#Complete linkage

table(sample_status, cutree(hc_complete, k = 2)) 

plot(hc_complete)
```

<mark>To test for whether or not the result depends on linkage types, we do:

```{r hiearchical clustering average}
hc_average <- hclust(correlation_d, method = "average")
#Average linkage
table(sample_status, cutree(hc_average, k = 2))

plot(hc_average)

hc_single <- hclust(correlation_d, method = "single")
#Single linkage
table(sample_status, cutree(hc_single, k = 2))

plot(hc_single)
```

<mark>We observed different results, thus it does depend on the type of linkage selected.

### (c) 

<mark>To give a best representation of which genes differ the most, we calculate the absolute mean value difference:

```{r abs mean diff}
rownames(data) = paste0("gene", 1:1000)
healthy_data <- data[, 1:20]
diseased_data <- data[, 21:40]

healthy_means <- rowMeans(healthy_data)
diseased_means <- rowMeans(diseased_data)


abs_mean_diff <- sort(abs(healthy_means - diseased_means),
                      decreasing = TRUE)[1:3]

healthy_var <- apply(healthy_data, 1, var)
diseased_var <- apply(diseased_data, 1, var)

abs_eff_size <- sort(abs(healthy_means - diseased_means) * sqrt(2) / sqrt(healthy_var + diseased_var), decreasing = TRUE)[1:3]

abs_mean_diff
abs_eff_size
```

<mark>From the result of `abs_mean_diff` and `abs_eff_size`, we can see that `gene600` are on the top of both lists, so they differ the most overall across the two groups, `healthy` and `diseased` individuals.
