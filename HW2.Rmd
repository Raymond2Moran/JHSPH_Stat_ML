---
title: "HW2_Statistical Machine Learning"
author: "Moran Guo"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1 (5.4-1)

*Pf.* Based on the property: $\mathrm{Var}(X + Y) = \mathrm{Var}(X) + \mathrm{Var}(Y) - 2\mathrm{Cov}(X, Y)$, thus we have $$\mathrm{Var}\Big(\alpha X + (1-\alpha)Y\Big) = \mathrm{Var}(\alpha X) + \mathrm{Var}\Big((1-\alpha) Y\Big) +2\mathrm{Cov}\Big(\alpha X, (1-\alpha)Y\Big).$$

Simplifying the equation using $\mathrm{Var}(aX) = a^2 \mathrm{Var}(X)$ and $\mathrm{Cov}(cX, dY) = cd \cdot \mathrm{Cov}(X, Y)$, we have $$\mathrm{Var}\Big(\alpha X + (1-\alpha)Y\Big) = \alpha^2 \mathrm{Var}(X) + (1 - \alpha)^2\mathrm{Var}(Y) + 2\alpha(1-\alpha)\cdot \mathrm{Cov}(X,Y).$$

We find the first derivative of the above equation with respect to $\alpha$, and let it equal to $0$ to find the extremum:

$$2\alpha\mathrm{Var}(X) + (-2)(1-\alpha)\mathrm{Var}(Y) + 2(1-2\alpha)\mathrm{Cov}(X, Y) = 0 \Rightarrow \alpha = \frac{\mathrm{Var}(Y) - \mathrm{Cov}(X,Y)}{\mathrm{Var}(X) + \mathrm{Var}(Y) - 2\mathrm{Cov}(X,Y)}.$$

To verify this extremum is a minimum, we differentiate it again with respect to $\alpha$, and we have $$2\mathrm{Var}(X) + 2\mathrm{Var}(Y) - 4\mathrm{Cov}(X,Y) = \mathrm{Var}(X) + \mathrm{Var}(Y) - 2\mathrm{Cov}(X,Y) = \mathrm{Var}(|X-Y|) \geq 0,$$

<mark> Therefore, we confirm that this $\alpha$ minimizes $\mathrm{Var}\Big(\alpha X + (1-\alpha)Y\Big)$.

## Question 2 (5.4-3)

### (a)

<mark> For the entire dataset, $k$-fold cross validation is implemented by dividing it into $k$ parts of equal size. The model will be fitted $k$ times, with one of the $k$ subsets serving as the test set and the remaining $k-1$ subsets forming training set. The average error across $k$ trials is computed at the end.

### (b)

#### (i)

<mark> Advantage of $k$-fold CV compared to validation set:

1.  More efficient use of the dataset, since the data is splitted into $k$ folds and the model is fitted $k$ times with different combination of folds for training and validation. However, for the validation set approach, the data is only splitted into one validation set and one training set, meaning the model is only trained on part of the data, which is less efficient.

2.  Less variance in model error. $k$-fold CV often results in less variance in performance estimates, since the model was fitted $k$ times and the results were averaged across multiple validation sets, thus the output error is more robust and less dependent on a single split. Whereas validation set approach only fit the model once, and if the validation set split happens to contain more extreme data points which are harder to fit, then the performance of the model may be affected by those data points.

<mark> Disadvantage of $k$-fold CV compared to validation set:

1.  Higher computational cost and more complex to implement. It is trivial, since for $k$-fold CV, the model need to be fitted and trained $k$ times, which can be particular challenging for large datasets and/or complex models. But validation set approach only need to train the model once, which requires less computational costs.

#### (ii)

<mark> Advantage of $k$-fold CV compared to LOOCV:

1.  Lower computational cost compared to LOOCV. Since LOOCV is essentially $n$-fold CV, when $n$ is large (e.g., $n > 10$), the model need to be fitted and trained $n$ times, as compared to $k$-fold CV, with $k$ normally equals to $5$ or $10$. Therefore LOOCV is more computationally intensive compared to $k$-fold CV, especially when $n$ is large.

2.  More control over bias-variance tradeoff. $k$-fold CV allows us to control the value of $k$ to account for an adequate level of variance and bias. However, LOOCV always uses $n-1$ data points for training, which often time result in low bias but high variance, especially for small datasets. This is due to the fact that for LOOCV, the overlap between training sets is large, and these highly correlated quantities often have high variances compared to those quantities that are not as much correlated, which occurs in $k$-fold CV.

<mark> Disadvantage of $k$-fold CV compared to LOOCV:

1.  Slightly higher bias compared to LOOCV. Since $k$-fold CV utilizes smaller training set for every fold, it will results in slightly higher bias than the LOOCV, which uses almost the entire dataset for training.

2.  Not optimal for small dataset compared to LOOCV. When the total number of observation $n$ is small, LOOCV could perform better than $k$-fold CV because it utilizes nearly all available data for training, where as $k$-fold CV only trains the model using $(k-1)/k$ of the data, which could limit the model performance when $n$ is small.

## Question 3 (5.4-5)

### (a)

```{r default logistic regression}
# Install required packages
library(ISLR)

set.seed(644)

default_glm_fit <- glm(default ~ income + balance, data = Default, family = "binomial")

summary(default_glm_fit)
```

### (b)

```{r default validation set}
set.seed(644)

# Train-test split
train_set <- sample(nrow(Default), nrow(Default) * 0.80)

# Model fit
default_glm_fit_vs <- glm(default ~ income + balance, data = Default, family = "binomial", subset = train_set)

# Predict on the test set
default_prob_vs <- predict(default_glm_fit_vs, Default[-train_set, ], type = "response")

# Assign to the predicted status
default_pred_vs <- ifelse(default_prob_vs > 0.5, "Yes", "No")

table(default_pred_vs, Default[-train_set, ]$default)

# Validation set error
mean(Default[-train_set, ]$default != default_pred_vs)

```

### (c)

```{r multi seed validation set}
for (i in 15:17) {
  set.seed(i)
  train_set <- sample(nrow(Default), nrow(Default) * 0.80)

  default_glm_fit_vs <- glm(default ~ income + balance, data = Default, family = "binomial", subset = train_set)

  default_prob_vs <- predict(default_glm_fit_vs, Default[-train_set, ], type = "response")

  default_pred_vs <- ifelse(default_prob_vs > 0.5, "Yes", "No")

  #table(default_pred_vs, Default[-train_set, ]$default)

  print(mean(Default[-train_set, ]$default != default_pred_vs))
  
}

```

<mark> From the output test errors, we may safely assume that our method may not have much variance.

### (d)

```{r default glm with dummy}
set.seed(644)

# Train-test split
train_set <- sample(nrow(Default), nrow(Default) * 0.80)

# Model fit with `student` as dummy
default_glm_fit_vs <- glm(default ~ income + balance + student, data = Default, family = "binomial", subset = train_set)

# Predict on the test set
default_prob_vs <- predict(default_glm_fit_vs, Default[-train_set, ], type = "response")

# Assign to the predicted status
default_pred_vs <- ifelse(default_prob_vs > 0.5, "Yes", "No")

table(default_pred_vs, Default[-train_set, ]$default)

# Validation set error
mean(Default[-train_set, ]$default != default_pred_vs)


```

<mark> From the result, we can see that `student` as a dummy variable does not changes the test error too much.

## Question 4 (5.4-6)

### (a)

```{r standard error}
library(ISLR)

set.seed(644)

default_glm_fit <- glm(default ~ income + balance, data = Default, family = "binomial")

summary(default_glm_fit)
```

<mark> We conclude from the table that the standard errors correspond to `income` and `balance` equal to $4.985e-06$ and $2.274e-04$ respectively.

### (b)

```{r bootfn}
boot_fn = function(data = Default, index) {
  return(coef(glm(default ~ income + balance,
                  data, 
                  family = binomial, 
                  subset = index))[c("income", "balance")])
}

```

### (c)

```{r boot with bootfn}
library(boot)

boot(Default, boot_fn, R = 100)

```

### (d)

<mark> The standard errors from the bootstrapping and from the previous `glm()` call are very similar.

## Question 5 (6.8-1)

### (a)

<mark> Best subset model with $k$ predictors has the smallest training error.

### (b)

<mark> We don't know any information regarding the test set, so it could be any of these three model.

### (c)

#### i.

<mark> True. This is the definition of forward stepwise selection.

#### ii.

<mark> True. This is the definition of backward stepwise selection.

#### iii.

<mark> False. The predictors of backward / forward stepwise selection at a particular step depends on the previous stage of the selection process. It is not dependent on the other selection strategy.

#### iv.

<mark> False. Reason is the same as iii. â€” The predictors of backward / forward stepwise selection at a particular step depends on the previous stage of the selection process. It is not dependent on the other selection strategy. So there's no guarantee that one strategy (forward / backward selection) has a relationship with the other.

#### v.

<mark> False. The selection of variables using the best subset model **solely** depends on the subset that minimizes RSS on the training set. So it is not necessarily nested for $k$ and $k+1$ predictors.

## Question 6 (6.8-8)

### (a)

```{r simulate normal}
set.seed(567)

x <- rnorm(100)
error <- rnorm(100)
```

### (b)

```{r response vector}
Y <- 4 + (0.5) * x + 3 * x^2 + 1 * x^3 + error

plot(x, Y)
```

### (c)

```{r best subset selection, echo=FALSE}
# Load required packages
library(leaps)

x_vec <- poly(x, 10, raw = TRUE)

colnames(x_vec) <- paste0("X", 1:10)

reg_all <- regsubsets(Y ~ ., data = data.frame(Y, x_vec), nvmax = 10)
reg_summary <- summary(reg_all)

# Plots

min_cp <- which.min(reg_summary$cp)

plot(reg_summary$cp, xlab = "# of Polynomial Terms", ylab = "C_p", type = "l")
points(min_cp, reg_summary$cp[min_cp], col = "red", pch = 2, lwd = 3)

min_bic <- which.min(reg_summary$bic)

plot(reg_summary$bic, xlab = "Number of Polynomial terms", ylab = "BIC", type = "l")

points(min_bic, reg_summary$bic[min_bic], col = "red", pch = 2, lwd = 3)

min_adj_R2 <- which.max(reg_summary$adjr2)  
plot(reg_summary$adjr2, xlab = "Number of Polynomial terms",
     ylab = "Adjusted R2", type = "l")
points(min_adj_R2, reg_summary$adjr2[min_adj_R2],
       col = "red", pch = 2, lwd = 3)

coef(reg_all, min_bic)
#coef(reg_all, min_cp)
#coef(reg_all, min_adj_R2)

```

<mark> From the result, we can see that adjusted $R^2$, $BIC$, and $C_p$ all choose the 4-variable model. Therefore, the best model is the 4 variables listed above.

### (d)

```{r forward and backward selection}
reg_forward <- regsubsets(Y ~ ., data = data.frame(Y, x_vec), nvmax = 10, method = "forward")
forward_summary <- summary(reg_forward)

reg_backward <- regsubsets(Y ~ ., data = data.frame(Y, x_vec), nvmax = 10, method = "backward")
backward_summary <- summary(reg_backward)

# Plots
par(mfrow = c(2,3))

min_cp <- which.min(forward_summary$cp)

plot(forward_summary$cp, xlab = "# of Polynomial Terms", ylab = "Forward Stepwise C_p", type = "l")
points(min_cp, forward_summary$cp[min_cp], col = "red", pch = 2, lwd = 3)

min_bic <- which.min(forward_summary$bic)

plot(forward_summary$bic, xlab = "Number of Polynomial terms", ylab = "Forward Stepwise BIC", type = "l")

points(min_bic, forward_summary$bic[min_bic], col = "red", pch = 2, lwd = 3)

min_adj_R2 <- which.max(forward_summary$adjr2)  
plot(forward_summary$adjr2, xlab = "Number of Polynomial terms",
     ylab = "Forward Stepwise Adjusted R2", type = "l")
points(min_adj_R2, forward_summary$adjr2[min_adj_R2],
       col = "red", pch = 2, lwd = 3)

min_cp <- which.min(backward_summary$cp)

plot(backward_summary$cp, xlab = "# of Polynomial Terms", ylab = "Backward Stepwise C_p", type = "l")
points(min_cp, backward_summary$cp[min_cp], col = "red", pch = 2, lwd = 3)

min_bic <- which.min(backward_summary$bic)

plot(backward_summary$bic, xlab = "Number of Polynomial terms", ylab = "Backward Stepwise BIC", type = "l")

points(min_bic, backward_summary$bic[min_bic], col = "red", pch = 2, lwd = 3)

min_adj_R2 <- which.max(backward_summary$adjr2)  
plot(backward_summary$adjr2, xlab = "Number of Polynomial terms",
     ylab = "Backward Stepwise Adjusted R2", type = "l")
points(min_adj_R2, forward_summary$adjr2[min_adj_R2],
       col = "red", pch = 2, lwd = 3)

# Coefficients

coef(reg_forward, which.min(forward_summary$cp))
coef(reg_forward, which.min(forward_summary$bic))
coef(reg_forward, which.max(forward_summary$adjr2))
coef(reg_backward, which.min(backward_summary$cp))
coef(reg_backward, which.min(backward_summary$bic))
coef(reg_backward, which.max(backward_summary$adjr2))
```

<mark> Based on the results, for choosing the best model, the $BIC$ of forward selection gets only three variables, where as the adjusted $R^2$ and $C_p$ of forward selection gets two identical $5$-variable models. The backward selection model chooses a different set of $4$-variable model. All of the models selected by forward & backward selection are different from the previous best subset model.

### (e)

```{r lasso fit}
# Load required packages
library(glmnet)

par(mfrow = c(1, 1))
x_matrix <- as.matrix(x_vec)
lasso_fit <- cv.glmnet(x_matrix, Y, alpha = 1, nfolds = 10)
lambda <- lasso_fit$lambda.min
plot(lasso_fit)

predict(lasso_fit, s = lambda, type = "coefficients")
```

<mark> From the summary result, we can see that Lasso model chooses the best model, which is the same as the previous forward stepwise selection $BIC$ model.

### (f)

```{r lasso 2}
# Define Y_2
Y_2 <- 1 + 2 * x^7 + error

reg_2_all <- regsubsets(Y_2 ~ ., data = data.frame(Y_2, x_vec), nvmax = 10)

par(mfrow = c(1, 3))
reg_2_summary <- summary(reg_2_all)

# Plots

min_cp <- which.min(reg_2_summary$cp)

plot(reg_2_summary$cp, xlab = "# of Polynomial Terms", ylab = "C_p of Y_2", type = "l")
points(min_cp, reg_2_summary$cp[min_cp], col = "red", pch = 2, lwd = 3)

min_bic <- which.min(reg_2_summary$bic)

plot(reg_2_summary$bic, xlab = "Number of Polynomial terms", ylab = "BIC of Y_2", type = "l")

points(min_bic, reg_2_summary$bic[min_bic], col = "red", pch = 2, lwd = 3)

min_adj_R2 <- which.max(reg_2_summary$adjr2)  
plot(reg_2_summary$adjr2, xlab = "Number of Polynomial terms",
     ylab = "Adjusted R2 of Y_2", type = "l")
points(min_adj_R2, reg_2_summary$adjr2[min_adj_R2],
       col = "red", pch = 2, lwd = 3)

# Coefficients

coef(reg_2_all, min_cp)
coef(reg_2_all, min_bic)
coef(reg_2_all, min_adj_R2)

# Lasso regression

par(mfrow = c(1,1))

lasso_fit_2 <- cv.glmnet(x_matrix, Y_2, alpha = 1)
lambda <- lasso_fit_2$lambda.min
plot(lasso_fit_2)

predict(lasso_fit_2, s = lambda, type = "coefficients")
```

<mark> The $BIC$ based on the best subset selection model AND the Lasso model identified the correct model. The best subset selection based on adjusted $R^2$ included too many variables.

## Question 7 (6.8-9)

### (a)

```{r train test split}
# Load required packages
library(ISLR)

set.seed(789)
train_ind <- sample(1:nrow(College), nrow(College) * 0.80)
train_set <- College[train_ind, ]
test_set <- College[-train_ind, ]
```

### (b)

```{r fit linear model}
lm_fit <- lm(Apps ~ ., data = train_set)
lm_pred <- predict(lm_fit, test_set)
lm_rmse <- sqrt(mean((test_set$Apps - lm_pred)^2))
lm_rmse
```

<mark> Therefore, the RMSE on the test set is $1079.981$.

### (c)

```{r fit ridge}
# Load required packages
library(glmnet)

xm_train <- model.matrix(Apps ~ ., data = train_set)[, -1]

xm_test <- model.matrix(Apps ~ ., data = test_set)[, -1]

ridge_fit <- cv.glmnet(xm_train, train_set$Apps, alpha = 0) # Set `alpha = 0` to ensure a ridge fit

lambda <- ridge_fit$lambda.min

ridge_pred <- predict(ridge_fit, s = lambda, newx = xm_test)

ridge_rmse <- sqrt(mean((test_set$Apps - ridge_pred)^2))

ridge_rmse
```

<mark> Thus, the RMSE on the test set for Ridge regression is $1137.579$.

### (d)

```{r lasso fit again}
lasso_fit_3 <- cv.glmnet(xm_train, train_set$Apps, alpha = 1) # Set `alpha = 1` to ensure a lasso fit

lambda <- lasso_fit_3$lambda.min

lasso_pred <- predict(lasso_fit_3, s = lambda, newx = xm_test)

lasso_rmse <- sqrt(mean((test_set$Apps - lasso_pred)^2))

lasso_rmse

lasso_coefs <- predict(lasso_fit_3, type = "coefficients", s = lambda)[1:ncol(College), ]

lasso_coefs[lasso_coefs != 0]
```

<mark> The RMSE on the test set of Lasso regression is $1087.336$. There are 16 non-zero coefficient estimates (including the intercept).

### (e)

```{r PCR fit}
# Load required packages
library(pls)

set.seed(644)

pcr_fit <- pcr(Apps ~ ., data = train_set, scale = TRUE, validation = "CV")

validationplot(pcr_fit, val.type = "RMSEP")

summary(pcr_fit)

error_cv <- RMSEP(pcr_fit)$val[1, , ]

min_cv <- which.min(error_cv) - 1 # min_cv = 17

pcr_pred <- predict(pcr_fit, test_set, ncomp = min_cv)

pcr_rmse <- sqrt(mean((test_set$Apps - pcr_pred)^2))

pcr_rmse
```

<mark> The value of $M$ selected by CV is $17$; the RMSE on the test set for PCR fit is $1079.981$.

### (f)

```{r PLS fit}
set.seed(644)

pls_fit <- plsr(Apps ~ ., data = train_set, scale = TRUE, validation = "CV")

validationplot(pls_fit, val.type = "RMSEP")

summary(pls_fit)

error_cv <- RMSEP(pls_fit)$val[1, , ]

min_cv <- which.min(error_cv) - 1 # min_cv = 16

pls_pred <- predict(pls_fit, test_set, ncomp = min_cv)

pls_rmse <- sqrt(mean((test_set$Apps - pls_pred)^2))

pls_rmse
```

<mark> The value of $M$ selected by CV is $16$; the RMSE on the test set is $1079.969$.

### (g)

```{r all errors}
error_all <- c(lm_rmse, ridge_rmse, lasso_rmse, pcr_rmse, pls_rmse)

names(error_all) <- c("lm", "ridge", "lasso", "pcr", "pls")

error_all
```

<mark> It seems that Ridge regression performed a little bit off compared to the rest of the models. PCR & PLS model seem to perform equally well compared to the simple linear model.
