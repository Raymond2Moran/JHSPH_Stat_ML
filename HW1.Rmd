---
title: "HW1_Statistical Machine Learning"
author: "Moran Guo"
date: "2024-09-08"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Question 1 (3.7-5)**

We have $\displaystyle \hat{y}_i = x_i \hat{\beta} = x_i \Big(\frac{\sum_{i'=1}^nx_{i'}y_{i'}}{\sum_{j = 1}^n x_j^2}\Big)$ so that $\displaystyle \hat{y}_i = \sum_{i'=1}^n \Big(\frac{x_i x_{i'}}{\sum_{j=1}^n x_j^2}\Big)y_i'$, therefore $$a_i' = \frac{x_ix_i'}{\sum_{j=1}^n x_j^2}.$$

**Question 2 (3.7-6)**

We have the least-square linear regression as $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$, we plug in $x = \bar{x}$ and $\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$ then we have $$\hat{y} = \bar{y} - \hat{\beta}_1 \bar{x} + \hat{\beta}_1 \bar{x} = \bar{y}.$$

Therefore, the point $(\bar{x}, \bar{y})$ must be on the least-square line.

**Question 3 (3.7-9)**

```{r auto scatterplot}
# (a) Scatterplot Matrix
# install.packages("ISLR")
library(ISLR)
data(Auto)
plot(Auto, cex = 0.4, col = "lightblue")
```

```{r correlation}
# (b) correlation matrix
cor(Auto[, 1:8]) # Excluded the first variable 'name'
```

```{r multiple linear regression}
# (c) multiple linear regression
car_lm_fit <- lm(mpg ~ . - name, data = Auto)
summary(car_lm_fit)
```

(i) From the result, we see a significant *p*-value and a sufficiently large F statistics for the overall model fit, which suggests that there exist a linear relationship between mpg and other predictors.

(ii) From the above linear model fit summary, we see variables 'weight', 'year', and 'origin' are the most significant predictors.

(iii) The coefficient of 'year' is statistical significant and positive, which suggests that year is positively associated with the outcome variable mpg. This implies that as time goes by, the car manufacturers improved cars' mpg, possibly by innovating new technology.

```{r diagnostic plot}
# (d) diagnostic plots
plot(car_lm_fit)
```

From the diagnostic plots (especially the Residuals vs. Leverage plot), we can see that observation 327, 394 seems to have a large residuals compared to the rest, which suggests that they are potentially outliers. Observation 14 has a large leverage value so that it is a point of large impact.

```{r update model with interactions}
# (e) linear regression w/ interaction effects
summary(update(car_lm_fit, . ~ . + horsepower:acceleration))
summary(update(car_lm_fit, . ~ . + horsepower:weight))
```

Intuitively, we might think that the interactions between horsepower & acceleration **and** horsepower & car weight may have significant influence on car's mpg.

From the result of linear model fit with added interaction terms, we can see that as acceleration increases, the effect of horsepower on car's mpg becomes negative with statistical significance. Cars with higher acceleration and higher horsepower will tend to have even lower mpg than would be expected from each of these two variables individually.

For the interactions between horsepower and car weight, we see a significantly positive effect, whereas car weight & horsepower on their own have negative effects on car mpg. This suggests that as weight increases, the negative effect of horsepower on car's mpg is moderated. Or in other words, the drop of fuel efficiency of high horsepower car is less severe for high weight cars compared to lighter cars.

```{r different transformations}
# (f) different transformation of variables

# log transform of horsepower

car_lm_fit_log_hp <- lm (mpg ~ . + log(horsepower) - name, data = Auto)
summary(car_lm_fit_log_hp)

# quadratic transform of horsepower

car_lm_fit_sq_hp <- lm (mpg ~ . + I((horsepower)^2) - name, data = Auto)
summary(car_lm_fit_sq_hp)
```

From the model fit results, we can see that both the natural log and the quadratic transform of variable 'horsepower' showed significant effects. We can also observe from the F-statistics that the model improved with the addition of transformed variables. So we would expect that horsepower has a non-linear relationship with mpg, which is the reason that adding transformed variables improves the overall model.

**Question 4 (3.7-15)**

```{r model fit}
# (a) simple regression model fit
#install.packages("MASS")
library(MASS)
predictors_bos <- names(Boston)[names(Boston) != "crim"] # Exclude response variable 'crim'

#Empty lists to store statistics
coef_l <- c()
pval_l <- c()
r2_l <- c()

#For loop, looping through all variables
for (i in predictors_bos) {
  formula <- as.formula(paste("crim ~", i))
  model <- lm(formula, data=Boston)
  model_summary <- summary(model)
  
  #Extract estimates
  coef <- model_summary$coefficients[2, 1]
  pval <- model_summary$coefficients[2, 4]
  r2 <- model_summary$r.squared             
  
  #Store the results in lists
  coef_l <- c(coef_l, coef)
  pval_l <- c(pval_l, pval)
  r2_l <- c(r2_l, r2)
}

#Create dataframe
summary_tab <- data.frame(
  Predictor = predictors_bos,
  Coefficient = coef_l,
  P_value = pval_l,
  R_squared = r2_l
)

# Display the table
summary_tab
```

From the integrated result summary table, we can observe that variables 'rad' and 'tax' have the most significant effect on the response variable, criminal rate per Capita.\

```{r multiple linear regression model fit}
# (b) multiple linear regression model fit

model_fit_multi_bos <- lm(crim ~ . , data = Boston)
summary(model_fit_multi_bos)
```

From the multiple linear regression result, it can be observed that variables 'dis' and 'rad' are the two most significant variables compared to the rest, with some of them have no significant effects on the response.

```{r compare coeffs}
# (c) Compare coefficients of two models

plot(summary_tab$Coefficient, model_fit_multi_bos$coefficients[-1], xlab = "Simple Linear Regression Coefficients", ylab = "Multiple Linear Regression Coefficients")
```

From the result we can see that the coefficient of variable 'nox' changed from around 31 in the simple linear regression model to around -10 in the multiple linear regression model. The coefficients of other variables also changed, but not as much as 'nox'.

```{r non-linear association}
# (d) test for non-linear association

predictors_compare_bos <- names(Boston)[names(Boston) != "crim"]

predictor_names <- c()
f_stats <- c()
p_value <- c()

for (i in predictors_compare_bos) {
  #Fit a simple linear model
  linear_model <- lm(as.formula(paste("crim ~", i)), data = Boston)
  #Fit a model with quadratic & cubic terms
  nonlinear_model <- lm(as.formula(paste("crim ~", i, "+ I(", i, "^2)", "+ I(", i, "^3)")),
  data = Boston)
  #Use ANOVA to compare the two models
  anova_result <- anova(linear_model, nonlinear_model)
  #Extract statistics
  f_stats_temp <- anova_result$F[2]
  p_value_temp <- anova_result$`Pr(>F)`[2]
  
  predictor_names <- c(predictor_names, i)
  f_stats <- c(f_stats, f_stats_temp)
  p_value <- c(p_value, p_value_temp)
  
}

#Create a data frame to store the results
comparison_tab <- data.frame(
  Predictors = predictor_names,
  F_statistics = f_stats,
  P_value = p_value
)

comparison_tab[order(comparison_tab$P_value), ]
```

To compare the results between a simple linear regression model and a third order model suggested by the question, we fit the two models separately and use ANOVA test to see if the third order fit is sufficiently larger than the model we get from a simple linear regression.

From the results, we can see that variables 'medv', 'dis', 'nox', 'indus', 'age', 'tax', 'ptratio', etc., have significant *P* values which suggests a significant non-linear relationship.

**Question 5 (4.7-1)**

We work from **Eq. (4.3)** back to **Eq. (4.2)** to show that they are equivalent:

$$
\frac{p(X)}{1 - p(X)} = e^{\beta_0 + \beta_1X} \Rightarrow p(X) = e^{\beta_0 + \beta_1X}(1 - p(X)) \Rightarrow p(X)(1 + e^{\beta_0 + \beta_1X}) = e^{\beta_0 + \beta_1X} \Rightarrow p(X) = \frac{e^{\beta_0 + \beta_1X}}{1+e^{\beta_0 + \beta_1X}}.
$$

**Question 6 (4.7-8)**

Note that for the KNN with $N = 1$, it will have a zero error rate for the training data set. So we must have $$\frac{0 + \text{ test error rate}}{2} = 0.18 \Rightarrow \text{ test error rate} = 0.36. $$

We have the information that logistic regression achieves a test error rate of $0.30$, so we would prefer logistic regression because of small error rate.

**Question 7 (4.7-10)**

```{r summary}
summary(Weekly)
```
