---
title: "HW1_Statistical Machine Learning"
author: "Moran Guo"
date: "2024-09-11"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Question 1 (3.7-5)**

We have $\displaystyle \hat{y}_i = x_i \hat{\beta} = x_i \Big(\frac{\sum_{i'=1}^nx_{i'}y_{i'}}{\sum_{j = 1}^n x_j^2}\Big)$ so that $\displaystyle \hat{y}_i = \sum_{i'=1}^n \Big(\frac{x_i x_{i'}}{\sum_{j=1}^n x_j^2}\Big)y_i'$, therefore $$a_i' = \frac{x_ix_i'}{\sum_{j=1}^n x_j^2}.$$

**Question 2 (3.7-6)**

We have the least-square linear regression as $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$, we plug in $x = \bar{x}$ and $\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$ then we have $$\hat{y} = \bar{y} - \hat{\beta}_1 \bar{x} + \hat{\beta}_1 \bar{x} = \bar{y}.$$

Therefore, the point $(\bar{x}, \bar{y})$ must be on the least-square line.

**Question 3 (3.7-9)**

**(a)**

```{r auto scatterplot}
# (a) Scatter Plot Matrix
# install.packages("ISLR")
library(ISLR)
data(Auto)
plot(Auto, cex = 0.4, col = "lightblue")
```

**(b)**

```{r correlation}
# (b) correlation matrix
cor(Auto[, 1:8]) # Excluded the first variable `name`
```

**(c)**

```{r multiple linear regression}
# (c) multiple linear regression
car_lm_fit <- lm(mpg ~ . - name, data = Auto)
summary(car_lm_fit)
```

<mark> (i) From the result, we see a significant *p*-value and a sufficiently large F statistics for the overall model fit, which suggests that there exist a linear relationship between mpg and other predictors.

<mark> (ii) From the above linear model fit summary, we see variables `weight`, `year`, and `origin` are the most significant predictors.

<mark> (iii) The coefficient of `year` is statistical significant and positive, which suggests that year is positively associated with the outcome variable mpg. This implies that as time goes by, the car manufacturers improved cars' mpg, possibly by innovating new technology.

**(d)**

```{r diagnostic plot}
# (d) diagnostic plots
plot(car_lm_fit)
```

<mark> From the diagnostic plots (especially the Residuals vs. Leverage plot), we can see that observation 327, 394 seems to have a large residuals compared to the rest, which suggests that they are potentially outliers. Observation 14 has a large leverage value so that it is a point of large impact.

**(e)**

```{r update model with interactions}
# (e) linear regression w/ interaction effects
summary(update(car_lm_fit, . ~ . + horsepower:acceleration))
summary(update(car_lm_fit, . ~ . + horsepower:weight))
```

<mark> Intuitively, we might think that the interactions between horsepower & acceleration **and** horsepower & car weight may have significant influence on car's mpg.

<mark> From the result of linear model fit with added interaction terms, we can see that as acceleration increases, the effect of horsepower on car's mpg becomes negative with statistical significance. Cars with higher acceleration and higher horsepower will tend to have even lower mpg than would be expected from each of these two variables individually.

<mark> For the interactions between horsepower and car weight, we see a significantly positive effect, whereas car weight & horsepower on their own have negative effects on car mpg. This suggests that as weight increases, the negative effect of horsepower on car's mpg is moderated. Or in other words, the drop of fuel efficiency of high horsepower car is less severe for high weight cars compared to lighter cars.

**(f)**

```{r different transformations}
# (f) different transformation of variables

# log transform of horsepower

car_lm_fit_log_hp <- lm (mpg ~ . + log(horsepower) - name, data = Auto)
summary(car_lm_fit_log_hp)

# quadratic transform of horsepower

car_lm_fit_sq_hp <- lm (mpg ~ . + I((horsepower)^2) - name, data = Auto)
summary(car_lm_fit_sq_hp)
```

<mark> From the model fit results, we can see that both the natural log and the quadratic transform of variable `horsepower` showed significant effects. We can also observe from the F-statistics that the model improved with the addition of transformed variables. So we would expect that horsepower has a non-linear relationship with mpg, which is the reason that adding transformed variables improves the overall model.

**Question 4 (3.7-15)**

**(a)**

```{r model fit}
# (a) simple regression model fit

#install.packages("MASS")
library(MASS)
predictors_bos <- names(Boston)[names(Boston) != "crim"] # Exclude response variable `crim`

#Empty lists to store statistics
coef_l <- c()
pval_l <- c()
r2_l <- c()

#For loop, looping through all variables
for (i in predictors_bos) {
  formula <- as.formula(paste("crim ~", i))
  model <- lm(formula, data=Boston)
  model_summary <- summary(model)
  
  #Extract estimates
  coef <- model_summary$coefficients[2, 1]
  pval <- model_summary$coefficients[2, 4]
  r2 <- model_summary$r.squared             
  
  #Store the results in lists
  coef_l <- c(coef_l, coef)
  pval_l <- c(pval_l, pval)
  r2_l <- c(r2_l, r2)
}

#Create dataframe
summary_tab <- data.frame(
  Predictor = predictors_bos,
  Coefficient = coef_l,
  P_value = pval_l,
  R_squared = r2_l
)

# Display the table
summary_tab
```

```{r backup plots}
par(mfrow = c(2, 2))  #Display 4 plots at once (2x2 grid)

for (i in predictors_bos) {
  formula <- as.formula(paste("crim ~", i))
  model <- lm(formula, data = Boston)
  
  #Create scatter plot with regression line
  plot(Boston[[i]], Boston$crim, 
       xlab = i, 
       ylab = "crim", 
       main = paste("crim vs", i), 
       pch = 19, 
       col = "lightblue")
  
  # Add regression line
  abline(model, col = "red", lwd = 2)
}



#Reset plotting area
par(mfrow = c(1, 1))  

#Log-transform the p-values
summary_tab$log_P_value <- -log10(summary_tab$P_value)


barplot(summary_tab$log_P_value,
        names.arg = summary_tab$Predictor,
        col = "lightblue",
        las = 2,
        main = "-log10(P-values) for Predictors",
        ylab = "-log10(P-value)",
        ylim = c(0, max(summary_tab$log_P_value) + 1))
```

<mark> From the integrated result summary table as well as the scatter plot AND the log-transformed *p*-value bar plots, we can observe that variables `rad` and `tax` have the most significant effect on the response variable, criminal rate per Capita.

**(b)**

```{r multiple linear regression model fit}
# (b) multiple linear regression model fit

model_fit_multi_bos <- lm(crim ~ . , data = Boston)
summary(model_fit_multi_bos)
```

<mark> From the multiple linear regression result, it can be observed that variables `dis` and `rad` are the two most significant variables compared to the rest, with some of them have no significant effects on the response.

**(c)**

```{r compare coeffs}
# (c) Compare coefficients of two models

plot(summary_tab$Coefficient, model_fit_multi_bos$coefficients[-1], xlab = "Simple Linear Regression Coefficients", ylab = "Multiple Linear Regression Coefficients")
```

<mark> From the result we can see that the coefficient of variable `nox` changed from around 31 in the simple linear regression model to around -10 in the multiple linear regression model. The coefficients of other variables also changed, but not as much as `nox`.

**(d)**

```{r non-linear association}
# (d) test for non-linear association

predictors_compare_bos <- names(Boston)[names(Boston) != "crim"]

predictor_names <- c()
f_stats <- c()
p_value <- c()

for (i in predictors_compare_bos) {
  #Fit a simple linear model
  linear_model <- lm(as.formula(paste("crim ~", i)), data = Boston)
  #Fit a model with quadratic & cubic terms
  nonlinear_model <- lm(as.formula(paste("crim ~", i, "+ I(", i, "^2)", "+ I(", i, "^3)")),
  data = Boston)
  #Use ANOVA to compare the two models
  anova_result <- anova(linear_model, nonlinear_model)
  #Extract statistics
  f_stats_temp <- anova_result$F[2]
  p_value_temp <- anova_result$`Pr(>F)`[2]
  
  predictor_names <- c(predictor_names, i)
  f_stats <- c(f_stats, f_stats_temp)
  p_value <- c(p_value, p_value_temp)
  
}

#Create a data frame to store the results
comparison_tab <- data.frame(
  Predictors = predictor_names,
  F_statistics = f_stats,
  P_value = p_value
)

comparison_tab[order(comparison_tab$P_value), ]
```

<mark> To compare the results between a simple linear regression model and a third order model suggested by the question, we fit the two models separately and use ANOVA test to see if the third order fit is sufficiently larger than the model we get from a simple linear regression.

<mark> From the results, we can see that variables `medv`, `dis`, `nox`, `indus`, `age`, `tax`, `ptratio`, etc., have significant *p*-values which suggests a significant non-linear relationship.

**Question 5 (4.7-1)**

<mark> We work from **Eq. (4.3)** back to **Eq. (4.2)** to show that they are equivalent:

$$
\frac{p(X)}{1 - p(X)} = e^{\beta_0 + \beta_1X} \Rightarrow p(X) = e^{\beta_0 + \beta_1X}(1 - p(X)) \Rightarrow p(X)(1 + e^{\beta_0 + \beta_1X}) = e^{\beta_0 + \beta_1X} \Rightarrow p(X) = \frac{e^{\beta_0 + \beta_1X}}{1+e^{\beta_0 + \beta_1X}}.
$$

**Question 6 (4.7-8)**

<mark> Note that for the KNN with $N = 1$, it will have a zero error rate for the training data set. So we must have $$\frac{0 + \text{ test error rate}}{2} = 0.18 \Rightarrow \text{ test error rate} = 0.36. $$

<mark> We have the information that logistic regression achieves a test error rate of $0.30$, so we would prefer logistic regression because of small error rate.

**Question 7 (4.7-10)**

**(a)**

```{r summary}
# (a) Summary and plots
library(MASS)
summary(Weekly)
plot(Weekly, cex = 0.3, col = "lightblue")
```

<mark> From the summary table and the plot, we can observe a relative balance distribution for all variables excluding `Year` and `Volume`, with mean values clustered around 0. The plot shows that there seems to be an exponential relationship between variable `Year` and `Volume`.

**(b)**

```{r logistic regression}
# (b) Logistic regression
weekly_lreg_fit <- glm(Direction ~ . - Today, data = Weekly, family = binomial)
summary(weekly_lreg_fit)
```

<mark> From the result of logistic regression, only variable `Lag2` is statistically significant.

**(c)**

```{r confusion matrix}
# (c) Construct confusion matrix

#Predict the outcomes based on predictors
pred_probs <- predict(weekly_lreg_fit, type = "response")
glm_pred <- rep("Down", length(pred_probs))
glm_pred[pred_probs > 0.5] <- "Up"

#Confusion matrix
table(glm_pred, Weekly$Direction)
```

<mark> From the table it seems that the model is predicting Up more frequently than predicting Down. So we could argue that this model tends to predict the market was up during this period of time. Another noticeable fact is that the overall correct classification rate is $\frac{56+558}{56+558+47+428} \approx 0.564$, which suggests a training error rate of $0.436$, which is pretty high considering that the training error is often-time overly optimistic.

**(d)**

```{r logistic regression lag2}
library(MASS)
# (d) Logistic regression using Lag2

#Specify test and train sets
test_set <- Weekly[Weekly$Year > 2008, ]
train_set <- Weekly[Weekly$Year <= 2008, ]

lag2_lreg_fit <- glm(Direction ~ Lag2, family = binomial, data = train_set)

lag_2_pred_probs <- predict(lag2_lreg_fit, newdata = test_set, type = "response")

lag_2_preds <- rep("Down", length(lag_2_pred_probs))

lag_2_preds[lag_2_pred_probs > 0.5] <- "Up"

table(lag_2_preds, test_set$Direction)
```

<mark> We can calculate the overall fraction of correct prediction is $\frac{9+56}{9+56+34+5} = 0.625$, improved from the previous model.

**(e)**

```{r LDA}
# (e) LDA model

lag_2_lda_fit <- lda(Direction ~ Lag2, data = train_set)
lag_2_lda_pred <- predict(lag_2_lda_fit, newdata = test_set, type = "response")$class
table(lag_2_lda_pred, test_set$Direction)
```

<mark> Similarly, we can calculate the overall correct prediction rate: $\frac{9+56}{9+56+34+5} = 0.625$.

**(f)**

```{r QDA}
# (f) QDA model

lag_2_qda_fit <- qda(Direction ~ Lag2, data = train_set)
lag_2_qda_pred <- predict(lag_2_qda_fit, newdata = test_set, type = "response")$class
table(lag_2_qda_pred, test_set$Direction)
```

The overall fraction of correct prediction is: $\frac{61}{61+43} = 0.587$.

**(g)**

```{r KNN}
# (g) KNN model with K = 1

library(class)
train_set_KNN <- cbind(train_set$Lag2)
test_set_KNN <- cbind(test_set$Lag2)
train_response_KNN <- train_set$Direction

pred_probs_KNN <- knn(train_set_KNN, test_set_KNN, train_response_KNN) #Default value is k=1

table(pred_probs_KNN, test_set$Direction)
```

<mark> The overall fraction of correct prediction for KNN with $K=1$ is $\frac{21+31}{21+31+30+22} = 0.50$.

**(h)**

<mark> Among the models tested above, logistic regression and LDA performed the best based on the overall correct prediction rates.

**(i)**

<mark> We try to vary the value of $K$ in the KNN model to see if the accuracy improves.

```{r modify model KNN}
# (i) Modify KNN's K values

#Define a range of K values to test
k_values <- c(3, 5, 7, 9)

for (k in k_values) {
  pred_probs_KNN <- knn(train_set_KNN, test_set_KNN, train_response_KNN, k = k)
  confusion_matrix <- table(pred_probs_KNN, test_set$Direction)
  
  cat("\nK =", k, "\n")
  print(confusion_matrix)
  
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  cat("Accuracy for K =", k, ":", accuracy, "\n")
}

```

```{r modify variables}
#We use only the first three lags to see if we can improve the accuracy
train_set_KNN <- cbind(train_set$Lag1, train_set$Lag2, train_set$Lag3)
test_set_KNN <- cbind(test_set$Lag1, test_set$Lag2, test_set$Lag3)

train_response_KNN <- train_set$Direction

k_values <- c(1, 3, 5, 7, 9)

for (k in k_values) {
  pred_probs_KNN <- knn(train_set_KNN, test_set_KNN, train_response_KNN, k = k)
  confusion_matrix <- table(pred_probs_KNN, test_set$Direction)
  
  cat("\nK =", k, "\n")
  print(confusion_matrix)
  
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  cat("Accuracy for K =", k, ":", accuracy, "\n")
}
```

<mark> To improve the model, I tried two strategies:

1.  <mark> Increase the value of $K$ in the KNN model. The result indicates that if we use all variables as predictors, $K = 3$ and $K = 9$ achieved the same accuracy around $0.548$.
2.  <mark> Use only the first three lags `Lag1`, `Lag2`, and `Lag3` to predict the results. The KNN model was constructed based on modified data frame and varied $K$'s. From the result, we can see that $K = 7$ in this setting performed the best.

**Question 8 (4.7-11)**

**(a)**

```{r auto01}
# (a) Create variable

library(ISLR)
Auto$mpg01 <- ifelse(Auto$mpg > median(Auto$mpg), 1, 0)
```

**(b)**

```{r correlation MPG}
# (b) Examine correlation
plot(Auto, cex = 0.4, col = "lightblue")


cor_mpg01 <- cor(Auto[, c(1:8, 10)])[-9, 9] 
# Exclude the `name` variable, extract the correlation values with respect to `mpg01`

# Display the result
cor_mpg01
```

<mark> From the result, it seems that weight, displacement, cylinders, and horsepower have a moderately strong negative correlation with cars' mpg (based on `mpg01`); while car's `year` & `origin` have a moderate positive correlation. The former is kind of self-explanatory, and we could explain the latter by the advancement of technology in car manufacturing.

**(c)**

```{r split}
# (c) Split data

set.seed(817) #Set seed for reproducibility

n <- nrow(Auto)

indices <- sample(1:n)

train_size <- round(0.8 * n) #A 80/20 split

train_ind <- indices[1:train_size]
test_ind <- indices[(train_size + 1):n]

train_data <- Auto[train_ind, c(-1, -9)]
test_data <- Auto[test_ind, c(-1, -9)]
```

**(d)**

```{r LDA MPG}
# (d) LDA model fit

library(MASS)

mpg01_lda_fit <- lda(mpg01 ~ . - acceleration, data = train_data)
lda_predictions <- predict(mpg01_lda_fit, newdata = test_data, type = "response")

predicted_classes <- lda_predictions$class
actual_classes <- test_data$mpg01

lda_test_error <- mean(predicted_classes != actual_classes)

print(paste("Test Error: ", round(lda_test_error, 4)))
```

<mark> Therefore, the error rate on test set is 0.1282.

**(e)**

```{r QDA MPG}
# (d) QDA model fit

library(MASS)

mpg01_qda_fit <- qda(mpg01 ~ . - acceleration, data = train_data)
qda_predictions <- predict(mpg01_qda_fit, newdata = test_data, type = "response")

predicted_classes <- qda_predictions$class
actual_classes <- test_data$mpg01

qda_test_error <- mean(predicted_classes != actual_classes)

print(paste("Test Error: ", round(qda_test_error, 4)))
```

<mark> The error rate of QDA on test set is 0.1026.

**(f)**

```{r Logistic MPG}

library(MASS)

mpg01_lreg_fit <- glm(mpg01 ~ . - acceleration, data = train_data, family = binomial)

lreg_pred <- ifelse(predict(mpg01_lreg_fit, test_data, type = "response") > 0.5, 1, 0)

actual_classes <- test_data$mpg01

lreg_test_error <- mean(lreg_pred != actual_classes)

print(paste("Test Error: ", round(lreg_test_error, 4)))
```

<mark> The error rate of logistic regression on test set is 0.1410.

**(g)**

```{r KNN MPG}
# (g) KNN model fit and output the best k

library(class)
set.seed(123)
result <- data.frame(k = 1:10, error = "") #Initialize dataframe

for (k in 1:10) {
  mpg_knn_pred <- knn(train = train_data[, c(-5, -6, -7, -8)],
                      test = test_data[, c(-5, -6, -7, -8)], 
                      cl = train_data$mpg01, k = k)
  result[k, 2] <- mean(mpg_knn_pred != test_data$mpg01)
}

result
```

<mark> Based on the result, $K = 3, 4, 6, 8$ yields the same lowest error rate. So a valid answer will be $K = 3$.
